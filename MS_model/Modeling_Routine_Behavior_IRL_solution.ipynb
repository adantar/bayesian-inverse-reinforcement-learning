{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T21:42:26.783524Z",
     "start_time": "2020-04-19T21:39:19.360945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "# Standard imports that we will need in the rest of the notebook.\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "\n",
    "# Discrete distributions and sampling\n",
    "from gym import Env, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "# Inverse reinforcement learning\n",
    "from reinforce_learning import vi_rational\n",
    "from inv_reinforce_learning_state_action import compute_s_a_visitations, vi_boltzmann, compute_D\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing\n",
    "import arviz as az\n",
    "\n",
    "# Bayesian inference\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from theano.compile.ops import as_op\n",
    "\n",
    "print('Loading')\n",
    "\n",
    "state_vis_freq = np.load('./data/state_vis_freq.npy')\n",
    "prob_action_given_state = np.load('./data/prob_action_given_state.npy')\n",
    "transition_proba = np.load('./data/transition_proba.npy')\n",
    "state_feature_name = np.load('./data/state_feature_name.npy', allow_pickle=True)\n",
    "action_feature_name = np.load('./data/action_feature_name.npy', allow_pickle=True)\n",
    "state_feature = np.load('./data/state_feature.npy')\n",
    "action_feature = np.load('./data/action_feature.npy')\n",
    "initial_state_dist = np.load('./data/initial_state_dist.npy')\n",
    "\n",
    "from environment import MDP, Environment\n",
    "\n",
    "environment = Environment(state_feature_name, action_feature_name, \n",
    "                          state_feature, action_feature, transition_proba, initial_state_dist)\n",
    "mdp = MDP(environment)\n",
    "\n",
    "mdp.valid_sa = mdp.T.sum(axis=2)==1\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T21:45:49.506459Z",
     "start_time": "2020-04-19T21:45:49.500377Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_trajectories(mdp, policy, timesteps=35, num_traj=50):\n",
    "    '''\n",
    "    Generates trajectories in the MDP given a policy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    policy : 2D numpy array\n",
    "        Array of shape (mdp.nS, mdp.nA), each value p[s,a] is the probability \n",
    "        of taking action a in state s.\n",
    "    timesteps : int\n",
    "        Length of each of the generated trajectories.\n",
    "    num_traj : \n",
    "        Number of trajectories to generate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    3D numpy array\n",
    "        Expert trajectories. \n",
    "        Dimensions: [number of traj, timesteps in the traj, 2: state & action].\n",
    "    '''\n",
    "    \n",
    "    trajectories = np.zeros([num_traj, timesteps, 2]).astype(int)\n",
    "    \n",
    "    s = mdp.reset()\n",
    "    for i in range(num_traj):\n",
    "        for t in range(timesteps):\n",
    "            action = np.random.choice(mdp.nA, p=policy[s, :])\n",
    "            trajectories[i, t, :] = [s, action]\n",
    "            s = mdp.step(action)\n",
    "        s = mdp.reset()\n",
    "    \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T21:46:08.300670Z",
     "start_time": "2020-04-19T21:46:08.291541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State feature names:  ['Gender:Male' 'Gender:Female' 'Age:Younger Than 30'\n",
      " 'Age:Between 30 To 60' 'Age:60 And Older' 'Current Daytime Interval:Wake'\n",
      " 'Current Daytime Interval:Morning' 'Current Daytime Interval:Afternoon'\n",
      " 'Current Daytime Interval:Evening' 'Current Daytime Interval:Bed'\n",
      " 'Current Interval Pain Score:Lower' 'Current Interval Pain Score:Normal'\n",
      " 'Current Interval Pain Score:Higher'\n",
      " 'Current Interval Pain Score:Not Recorded'\n",
      " 'Current Interval Fatigue Score:Lower'\n",
      " 'Current Interval Fatigue Score:Normal'\n",
      " 'Current Interval Fatigue Score:Higher'\n",
      " 'Current Interval Fatigue Score:Not Recorded'\n",
      " 'Last Interval Activity Bouts:Lower'\n",
      " 'Last Interval Activity Bouts:Normal'\n",
      " 'Last Interval Activity Bouts:Higher'\n",
      " 'Last Interval Activity Bouts:Not Recorded'\n",
      " 'Eod Positive Affect And Well-Being:Lower'\n",
      " 'Eod Positive Affect And Well-Being:Normal'\n",
      " 'Eod Positive Affect And Well-Being:Higher'\n",
      " 'Eod Positive Affect And Well-Being:Not Recorded']\n",
      "Action feature names:  ['Activitybouts:Lower' 'Activitybouts:Normal' 'Activitybouts:Higher'\n",
      " 'Activitybouts:Not Recorded' 'Next Interval Assessment Pain:Recorded'\n",
      " 'Next Interval Assessment Pain:Not Recorded'\n",
      " 'Next Interval Assessment Fatigue:Recorded'\n",
      " 'Next Interval Assessment Fatigue:Not Recorded'\n",
      " 'Next Interval Assessment Paw:Recorded'\n",
      " 'Next Interval Assessment Paw:Not Recorded']\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "n_traj=10000\n",
    "traj_len = 35\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "print('State feature names: ',  environment.state_feature_names)\n",
    "print('Action feature names: ',  environment.action_feature_names)\n",
    "# The \"true\" reward weights and the reward\n",
    "theta_state_expert = np.zeros(environment.state_feature_matrix.shape[1])\n",
    "theta_state_expert[24] = 1.0\n",
    "theta_action_expert = np.zeros(environment.action_feature_matrix.shape[1])\n",
    "theta_action_expert[[1,4,6,8]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T21:46:09.411080Z",
     "start_time": "2020-04-19T21:46:09.387547Z"
    }
   },
   "outputs": [],
   "source": [
    "r_s_expert = np.squeeze(np.asarray(np.dot(environment.state_feature_matrix, theta_state_expert)))\n",
    "r_a_expert = np.squeeze(np.asarray(np.dot(environment.action_feature_matrix, theta_action_expert)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T21:46:22.321881Z",
     "start_time": "2020-04-19T21:46:11.723351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State feature names:  ['Gender:Male' 'Gender:Female' 'Age:Younger Than 30'\n",
      " 'Age:Between 30 To 60' 'Age:60 And Older' 'Current Daytime Interval:Wake'\n",
      " 'Current Daytime Interval:Morning' 'Current Daytime Interval:Afternoon'\n",
      " 'Current Daytime Interval:Evening' 'Current Daytime Interval:Bed'\n",
      " 'Current Interval Pain Score:Lower' 'Current Interval Pain Score:Normal'\n",
      " 'Current Interval Pain Score:Higher'\n",
      " 'Current Interval Pain Score:Not Recorded'\n",
      " 'Current Interval Fatigue Score:Lower'\n",
      " 'Current Interval Fatigue Score:Normal'\n",
      " 'Current Interval Fatigue Score:Higher'\n",
      " 'Current Interval Fatigue Score:Not Recorded'\n",
      " 'Last Interval Activity Bouts:Lower'\n",
      " 'Last Interval Activity Bouts:Normal'\n",
      " 'Last Interval Activity Bouts:Higher'\n",
      " 'Last Interval Activity Bouts:Not Recorded'\n",
      " 'Eod Positive Affect And Well-Being:Lower'\n",
      " 'Eod Positive Affect And Well-Being:Normal'\n",
      " 'Eod Positive Affect And Well-Being:Higher'\n",
      " 'Eod Positive Affect And Well-Being:Not Recorded']\n",
      "Action feature names:  ['Activitybouts:Lower' 'Activitybouts:Normal' 'Activitybouts:Higher'\n",
      " 'Activitybouts:Not Recorded' 'Next Interval Assessment Pain:Recorded'\n",
      " 'Next Interval Assessment Pain:Not Recorded'\n",
      " 'Next Interval Assessment Fatigue:Recorded'\n",
      " 'Next Interval Assessment Fatigue:Not Recorded'\n",
      " 'Next Interval Assessment Paw:Recorded'\n",
      " 'Next Interval Assessment Paw:Not Recorded']\n",
      "My policy:\n",
      "[[0.00000000e+00 0.00000000e+00 3.28136632e-01 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.48881582e-05]\n",
      " [0.00000000e+00 0.00000000e+00 2.46962688e-01 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.12027790e-05]\n",
      " [0.00000000e+00 0.00000000e+00 6.51110581e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.95413273e-07]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 6.51652860e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.95867142e-07]\n",
      " [0.00000000e+00 0.00000000e+00 6.51652860e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.95867142e-07]\n",
      " [0.00000000e+00 0.00000000e+00 6.51652860e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.95867142e-07]]\n"
     ]
    }
   ],
   "source": [
    "V, Q, policy_expert = vi_boltzmann(mdp, gamma, r_s_expert, r_a_expert, traj_len)\n",
    "\n",
    "print('State feature names: ', environment.state_feature_names)\n",
    "print('Action feature names: ', environment.action_feature_names)\n",
    "# print(\"My  theta: \", theta_expert)\n",
    "print(\"My policy:\")\n",
    "print(policy_expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T21:46:44.904513Z",
     "start_time": "2020-04-19T21:46:26.143819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate expert trajectories using the given expert policy.\n",
    "trajectories = generate_trajectories(mdp, policy_expert, traj_len, n_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T21:28:35.814293Z",
     "start_time": "2020-04-02T21:28:35.773755Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"data/generated_behavior_instances.npy\", trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pycharm]",
   "language": "python",
   "name": "conda-env-pycharm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
